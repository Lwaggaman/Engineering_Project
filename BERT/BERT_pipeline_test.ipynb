{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7fc7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "from pymongo import MongoClient\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43a553f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, do_lower_case=True, strip_accents=False)\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "\n",
    "#inputs = tokenizer(question, context, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abeee858",
   "metadata": {},
   "outputs": [],
   "source": [
    "?tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45b7dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9054bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "?tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778e6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 'Quito, oficialmente San Francisco de Quito, es la capital de la República del Ecuador, de la Provincia de Pichincha y la capital más antigua de Sudamérica. Es la ciudad más poblada del Ecuador,​ con 2 millones de habitantes en el área urbana, y aproximadamente 3 millones en todo el Área metropolitana.'\n",
    "questions = ['Cuál es la capital de Ecuador?', 'Cuál es la ciudad más poblada?']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e04b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Cuál es la capital de Ecuador?\n",
      "Answer: Quito\n",
      "Question: Cuál es la ciudad más poblada?\n",
      "Answer: Quito\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc51ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.cc_db\n",
    "texts = list(db.norms.find({'text': {'$exists': True}}))[:2]\n",
    "#texts = texts[0]['text']\n",
    "text1, text2 = remove_singles(texts[0]['text']), remove_singles(texts[1]['text'])\n",
    "text = ''.join([text1, text2])\n",
    "q0 = 'Quién enviará declaratoria?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbfb2638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',,',\n",
       " 'OFICIO',\n",
       " 'N.sadea',\n",
       " 'Nl.',\n",
       " 'JH',\n",
       " '_r',\n",
       " 'UN',\n",
       " 'MAT.',\n",
       " 'Propuesta',\n",
       " 'de',\n",
       " 'norma',\n",
       " 'constitucional',\n",
       " 'o',\n",
       " '.7',\n",
       " 'o',\n",
       " ',edei',\n",
       " 'sobre',\n",
       " 'Estados',\n",
       " 'de',\n",
       " 'Excepción',\n",
       " 'Constitucional.',\n",
       " 'CONVENCIÓN',\n",
       " 'CONSTITUCIONAL',\n",
       " 'Santiago',\n",
       " 'de',\n",
       " 'Chile,',\n",
       " '01',\n",
       " 'de',\n",
       " 'febrero',\n",
       " 'de',\n",
       " '2022',\n",
       " 'DE',\n",
       " 'Raúl',\n",
       " 'Celis',\n",
       " 'Hernán',\n",
       " 'Larraín',\n",
       " 'Cristián',\n",
       " 'Monckeberg',\n",
       " 'y',\n",
       " 'Convencionales',\n",
       " 'Constituyentes',\n",
       " 'de',\n",
       " 'la',\n",
       " 'República',\n",
       " 'de',\n",
       " 'Chile',\n",
       " 'PARA',\n",
       " 'Sra.',\n",
       " 'María',\n",
       " 'Elisa',\n",
       " 'Quinteros',\n",
       " 'Presidenta',\n",
       " 'de',\n",
       " 'la',\n",
       " 'Convención',\n",
       " 'Constitucional',\n",
       " 'Por',\n",
       " 'medio',\n",
       " 'de',\n",
       " 'la',\n",
       " 'presente,',\n",
       " 'nos',\n",
       " 'dirigimos',\n",
       " 'a',\n",
       " 'usted',\n",
       " 'en',\n",
       " 'su',\n",
       " 'calidad',\n",
       " 'de',\n",
       " 'presidenta',\n",
       " 'de',\n",
       " 'la',\n",
       " 'Convención',\n",
       " 'y,',\n",
       " 'en',\n",
       " 'virtud',\n",
       " 'de',\n",
       " 'lo',\n",
       " 'dispuesto',\n",
       " 'en',\n",
       " 'los',\n",
       " 'artículos',\n",
       " '81',\n",
       " 'y',\n",
       " 'siguientes',\n",
       " 'del',\n",
       " 'Reglamento',\n",
       " 'General',\n",
       " 'de',\n",
       " 'esta',\n",
       " 'Convención',\n",
       " 'Constitucional,',\n",
       " 'para',\n",
       " 'presentar',\n",
       " 'una',\n",
       " 'iniciativa',\n",
       " 'de',\n",
       " 'norma',\n",
       " 'constitucional,',\n",
       " 'sobre',\n",
       " 'Estados',\n",
       " 'de',\n",
       " 'Excepción',\n",
       " 'Constitucional,',\n",
       " 'para',\n",
       " 'ser',\n",
       " 'derivada',\n",
       " 'a',\n",
       " 'la',\n",
       " 'Comisión',\n",
       " 'sobre',\n",
       " 'Sistema',\n",
       " 'Político,',\n",
       " 'Gobierno,',\n",
       " 'Poder',\n",
       " 'Legislativo',\n",
       " 'y',\n",
       " 'Sistema',\n",
       " 'Electoral..,',\n",
       " 'según',\n",
       " 'se',\n",
       " 'indica',\n",
       " 'a',\n",
       " 'continuación',\n",
       " '1.',\n",
       " 'Fundamentación',\n",
       " 'de',\n",
       " 'la',\n",
       " 'propuesta',\n",
       " 'La',\n",
       " 'presente',\n",
       " 'propuesta',\n",
       " 'de',\n",
       " 'norma',\n",
       " 'constitucional,',\n",
       " 'en',\n",
       " 'línea',\n",
       " 'con',\n",
       " 'el',\n",
       " 'constitucionalismo',\n",
       " 'comparado',\n",
       " 'y',\n",
       " 'nuestra',\n",
       " 'propia',\n",
       " 'tradición,',\n",
       " 'reconoce',\n",
       " 'que',\n",
       " 'existen',\n",
       " 'circunstancias',\n",
       " 'calificadas',\n",
       " 'bajo',\n",
       " 'las',\n",
       " 'cuáles',\n",
       " 'es',\n",
       " 'razonable',\n",
       " 'disponer',\n",
       " 'la',\n",
       " 'restricción',\n",
       " 'de',\n",
       " 'determinados',\n",
       " 'derechos',\n",
       " 'y',\n",
       " 'libertades',\n",
       " 'de',\n",
       " 'las',\n",
       " 'personas,',\n",
       " 'pero',\n",
       " 'que,',\n",
       " 'en',\n",
       " 'un',\n",
       " 'sistema',\n",
       " 'democrático,',\n",
       " 'esto',\n",
       " 'siempre',\n",
       " 'debe',\n",
       " 'ser',\n",
       " 'una',\n",
       " 'excepción,',\n",
       " 'la',\n",
       " 'que',\n",
       " 'por',\n",
       " 'regla',\n",
       " 'general',\n",
       " 'requerirá',\n",
       " 'el',\n",
       " 'concurso',\n",
       " 'de',\n",
       " 'los',\n",
       " 'poderes',\n",
       " 'ejecutivo',\n",
       " 'y',\n",
       " 'legislativo.',\n",
       " 'En',\n",
       " 'este',\n",
       " 'sentido,',\n",
       " 'proponemos',\n",
       " 'simplificar',\n",
       " 'los',\n",
       " 'Estados',\n",
       " 'de',\n",
       " 'Excepción',\n",
       " 'constitucional',\n",
       " 'regulados',\n",
       " 'en',\n",
       " 'la',\n",
       " 'Constitución',\n",
       " 'actual',\n",
       " 'y',\n",
       " 'aumentar',\n",
       " 'los',\n",
       " 'contrapesos',\n",
       " 'institucionales',\n",
       " 'previstos',\n",
       " 'para',\n",
       " 'su',\n",
       " 'aprobación,',\n",
       " 'término',\n",
       " 'y',\n",
       " 'control,',\n",
       " 'sin',\n",
       " 'obviar',\n",
       " 'las',\n",
       " 'particularidades',\n",
       " 'de',\n",
       " 'nuestro',\n",
       " 'país,',\n",
       " 'permanentemente',\n",
       " 'expuesto',\n",
       " 'a',\n",
       " 'desastres',\n",
       " 'naturales',\n",
       " 'y',\n",
       " 'los',\n",
       " 'mayores',\n",
       " 'riesgos',\n",
       " 'que',\n",
       " 'en',\n",
       " 'este',\n",
       " 'sentido',\n",
       " 'se',\n",
       " 'advierten',\n",
       " 'debido',\n",
       " 'a',\n",
       " 'su',\n",
       " 'vulnerabilidad',\n",
       " 'a',\n",
       " 'la',\n",
       " 'emergencia',\n",
       " 'climática.',\n",
       " '2.',\n",
       " 'Propuesta',\n",
       " 'de',\n",
       " 'norma',\n",
       " 'constitucional',\n",
       " 'Título',\n",
       " 'De',\n",
       " 'los',\n",
       " 'Estados',\n",
       " 'de',\n",
       " 'Excepción',\n",
       " 'Constitucional',\n",
       " 'Artículo',\n",
       " '1.',\n",
       " 'De',\n",
       " 'los',\n",
       " 'estados',\n",
       " 'de',\n",
       " 'excepción',\n",
       " 'constitucional.',\n",
       " 'El',\n",
       " 'estado',\n",
       " 'de',\n",
       " 'sitio',\n",
       " 'podrá',\n",
       " 'ser',\n",
       " 'declarado',\n",
       " 'en',\n",
       " 'caso',\n",
       " 'de',\n",
       " 'guerra',\n",
       " 'interna',\n",
       " 'o',\n",
       " 'exterior,',\n",
       " 'grave',\n",
       " 'conmoción',\n",
       " 'interior,',\n",
       " 'grave',\n",
       " 'alteración',\n",
       " 'del',\n",
       " 'orden',\n",
       " 'público',\n",
       " 'o',\n",
       " 'daño',\n",
       " 'para',\n",
       " 'la',\n",
       " 'seguridad',\n",
       " 'interior.',\n",
       " 'El',\n",
       " 'estado',\n",
       " 'de',\n",
       " 'emergencia',\n",
       " 'podrá',\n",
       " 'ser',\n",
       " 'declarado',\n",
       " 'cuando',\n",
       " 'las',\n",
       " 'condiciones',\n",
       " 'referidas',\n",
       " 'en',\n",
       " 'el',\n",
       " 'inciso',\n",
       " 'anterior',\n",
       " 'sean',\n",
       " 'menos',\n",
       " 'graves.',\n",
       " 'El',\n",
       " 'estado',\n",
       " 'de',\n",
       " 'catástrofe',\n",
       " 'podrá',\n",
       " 'ser',\n",
       " 'declarado',\n",
       " 'en',\n",
       " 'caso',\n",
       " 'de',\n",
       " 'calamidad',\n",
       " 'pública',\n",
       " 'y',\n",
       " 'lo',\n",
       " 'declarará',\n",
       " 'el',\n",
       " 'Presidente',\n",
       " 'de',\n",
       " 'la',\n",
       " 'República,',\n",
       " 'determinando',\n",
       " 'la',\n",
       " 'zona',\n",
       " 'afectada',\n",
       " 'por',\n",
       " 'la',\n",
       " 'misma.',\n",
       " 'Artículo',\n",
       " '2.',\n",
       " 'De',\n",
       " 'la',\n",
       " 'declaratoria.',\n",
       " 'El',\n",
       " 'Presidente',\n",
       " 'de',\n",
       " 'la',\n",
       " 'República',\n",
       " 'podrá',\n",
       " 'decretar',\n",
       " 'los',\n",
       " 'estados',\n",
       " 'de',\n",
       " 'sitio',\n",
       " 'o',\n",
       " 'de',\n",
       " 'emergencia',\n",
       " 'en',\n",
       " 'todo',\n",
       " 'el',\n",
       " 'territorio',\n",
       " 'nacional',\n",
       " 'o',\n",
       " 'en',\n",
       " 'parte',\n",
       " 'de',\n",
       " 'él.',\n",
       " 'Para',\n",
       " 'ello',\n",
       " 'enviará',\n",
       " 'la',\n",
       " 'declaratoria',\n",
       " 'de',\n",
       " 'estado',\n",
       " 'de',\n",
       " 'excepción',\n",
       " 'a',\n",
       " 'la',\n",
       " 'Cámara',\n",
       " 'de',\n",
       " 'Diputadas',\n",
       " 'y',\n",
       " 'Diputados',\n",
       " 'en',\n",
       " 'el',\n",
       " 'plazo',\n",
       " 'de',\n",
       " 'cuarenta',\n",
       " 'y',\n",
       " 'ocho',\n",
       " 'horas',\n",
       " 'desde',\n",
       " 'su',\n",
       " 'firma.',\n",
       " 'Esta',\n",
       " 'debatirá',\n",
       " 'inmediatamente',\n",
       " 'la',\n",
       " 'declaración',\n",
       " 'del',\n",
       " 'Presidente',\n",
       " 'y,',\n",
       " 'para',\n",
       " 'que',\n",
       " 'se',\n",
       " 'mantenga',\n",
       " 'en',\n",
       " 'vigencia,',\n",
       " 'deberá',\n",
       " 'ser',\n",
       " 'aprobada',\n",
       " 'por',\n",
       " 'la',\n",
       " 'mayoría',\n",
       " 'absoluta',\n",
       " 'de',\n",
       " 'las',\n",
       " 'diputadas',\n",
       " 'y',\n",
       " 'diputados',\n",
       " 'en',\n",
       " 'ejercicio.',\n",
       " 'La',\n",
       " 'declaratoria',\n",
       " 'tendrá',\n",
       " 'una',\n",
       " 'vigencia',\n",
       " 'máxima',\n",
       " 'de',\n",
       " 'sesenta',\n",
       " 'días',\n",
       " 'corridos,',\n",
       " 'la',\n",
       " 'que',\n",
       " 'podrá',\n",
       " 'prorrogarse',\n",
       " 'por',\n",
       " 'treinta',\n",
       " 'días',\n",
       " 'más,',\n",
       " 'con',\n",
       " 'la',\n",
       " 'aprobación',\n",
       " 'de',\n",
       " 'la',\n",
       " 'mayoría',\n",
       " 'absoluta',\n",
       " 'de',\n",
       " 'ambas',\n",
       " 'cámaras.',\n",
       " 'Futuras',\n",
       " 'prórrogas',\n",
       " 'de',\n",
       " 'los',\n",
       " 'estados',\n",
       " 'de',\n",
       " 'excepción',\n",
       " 'deberán',\n",
       " 'contar',\n",
       " 'con',\n",
       " 'el',\n",
       " 'voto',\n",
       " 'favorable',\n",
       " 'de',\n",
       " 'cuatro',\n",
       " 'séptimos',\n",
       " 'de',\n",
       " 'ambas',\n",
       " 'cámaras.',\n",
       " 'Si',\n",
       " 'el',\n",
       " 'Presidente',\n",
       " 'no',\n",
       " 'solicitara',\n",
       " 'la',\n",
       " 'renovación',\n",
       " 'de',\n",
       " 'la',\n",
       " 'declaratoria,',\n",
       " 'esta',\n",
       " 'se',\n",
       " 'entenderá',\n",
       " 'caducada.',\n",
       " 'Cuando',\n",
       " 'las',\n",
       " 'causas',\n",
       " 'que',\n",
       " 'hubieren',\n",
       " 'motivado',\n",
       " 'la',\n",
       " 'declaratoria',\n",
       " 'de',\n",
       " 'estado',\n",
       " 'de',\n",
       " 'excepción',\n",
       " 'no',\n",
       " 'subsistieran,',\n",
       " 'el',\n",
       " 'Presidente',\n",
       " 'de',\n",
       " 'la',\n",
       " 'República',\n",
       " 'decretará',\n",
       " 'su',\n",
       " 'término',\n",
       " 'y',\n",
       " 'notificará',\n",
       " 'al']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2.split()[:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a812f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Quién enviará declaratoria?', 'cámara de diputadas y diputados')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(q0, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bfeeb5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cded8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = inputs2['input_ids']\n",
    "?test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c1961878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2['attention_mask'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "53f84371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e125b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2 = tokenizer(q3, text3, add_special_tokens=True, return_tensors=\"pt\",  \n",
    "                    stride=20, padding='longest', truncation=True,\n",
    "                    max_length=512, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "5fc639d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    4,  1296,  5676,  ..., 17664, 30934,     5],\n",
       "        [    4,  1296,  5676,  ..., 26336,  1030,     5],\n",
       "        [    4,  1296,  5676,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bc35c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\",\n",
    "                   stride=20, padding=False, truncation=True,\n",
    "                    max_length=512, return_overflowing_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "45230285",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2302679218.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/tv/8qzgqvq554vgrs4zb6bwbwlm0000gn/T/ipykernel_2631/2302679218.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    best_score =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def qa(question, context):\n",
    "    chunks = tokenizer(question, context, add_special_tokens=True,\n",
    "                       return_tensors=\"pt\", padding=True,\n",
    "                       truncation=True, max_length=512, return_overflowing_tokens=True)\n",
    "    best_score = \n",
    "    best_\n",
    "    for i in len(inputs['iput_ids']):\n",
    "        input_ids = inputs[\"input_ids\"][i].tolist()[0]\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        answer_start_scores = outputs.start_logits\n",
    "        answer_end_scores = outputs.end_logits\n",
    "\n",
    "        answer_start = torch.argmax(answer_start_scores)\n",
    "        answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "        answer = tokenizer.convert_tokens_to_string(\n",
    "            tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "        )\n",
    "        return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_ga(question, context):\n",
    "    chunks = tokenizer(question, context, add_special_tokens=True,\n",
    "                       return_tensors=\"pt\", stride=20, padding=True,\n",
    "                       truncation=True, max_length=512, return_overflowing_tokens=True)\n",
    "    best_score = \n",
    "    best_\n",
    "    for i in range(len(inputs['iput_ids'])):\n",
    "        input_ids = inputs[\"input_ids\"][i].tolist()[0]\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        answer_start_scores = outputs.start_logits\n",
    "        answer_end_scores = outputs.end_logits\n",
    "\n",
    "        answer_start = torch.argmax(answer_start_scores)\n",
    "        answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "        answer = tokenizer.convert_tokens_to_string(\n",
    "            tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "        )\n",
    "        return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9de86d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ' EE TAnaME    Patricia Labra B Álvaro Jofré  Paulina  N JÚ   N   & # Bárbara Bouchon N   MAMIK   f D  c A  o   o N al y   o    v AFE  qu p á kát    Bernardo Font'\n",
    "def remove_singles(string):\n",
    "    string = re.sub(r'[^\\w\\s\\.\\,]', r'', string) #remove spec chars\n",
    "    string = re.sub(r'\\sy\\s', r' $& ', string) #Hide 1-digit chars\n",
    "    string = re.sub(r'\\so\\s', r' or ', string)\n",
    "    string = re.sub(r'\\se\\s', r' edei ', string)\n",
    "    string = re.sub(r'\\sa\\s', r' adea ', string)\n",
    "    string = re.sub(r'\\su\\s', r' udeo ', string)\n",
    "    string = re.sub(r'\\s.\\s', r'', string) #remove 1-digit words\n",
    "    string = re.sub(r'\\s\\$&\\s', r' y ', string) #recover 1-digit chars\n",
    "    string = re.sub(r'\\sor\\s', r' o ', string)\n",
    "    string = re.sub(r'\\sedei\\s', r' e ', string)\n",
    "    string = re.sub(r'\\sadea\\s', r' a ', string)\n",
    "    string = re.sub(r'\\sudeo\\s', r' u ', string)\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8c2a39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = text2[:-450] + '. El asado sucede el Viernes.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e820b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa(q0, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e7ecd621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Cuál es la ciudad más poblada?',\n",
       " 'Quito, oficialmente San Francisco de Quito, es la capital de la República del Ecuador, de la Provincia de Pichincha y la capital más antigua de Sudamérica. Es la ciudad más poblada del Ecuador,\\u200b con 2 millones de habitantes en el área urbana, y aproximadamente 3 millones en todo el Área metropolitana.')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7558e772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs2.sequence_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499df93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids', 'token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6bde2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = re.sub(r'[^\\w\\s]', r'', text3)\n",
    "q3 = 'Qué sucede el viernes?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ef975f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(q3, text3, add_special_tokens=True, return_tensors=\"pt\",  \n",
    "                    stride=40, padding='longest', truncation='only_second',\n",
    "                    max_length=512, return_overflowing_tokens=True,\n",
    "                  return_offsets_mapping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "41c924e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping']) <class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inputs['input_ids'])):\n",
    "    get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_chunker(question, context):\n",
    "    inputs2 = tokenizer(q0, text2, add_special_tokens=True, return_tensors=\"pt\",  \n",
    "                    stride=40, padding='longest', truncation='only_second',\n",
    "                    max_length=512, return_overflowing_tokens=True)\n",
    "    for i in \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "961fb7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score2(inputs, question, context):\n",
    "    #inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True\n",
    "    #                   stride=20, return_overflowing_tokens=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    sequence_ids = inputs.sequence_ids()\n",
    "\n",
    "    # Mask out-of-context tokens\n",
    "    mask = [i != 1 for i in sequence_ids]\n",
    "\n",
    "    # Unmask the [CLS] token\n",
    "    mask[0] = False\n",
    "    mask = torch.tensor(mask)[None]\n",
    "\n",
    "    # Neg large nr's exp will be zero\n",
    "    start_logits[mask] = -10000\n",
    "    end_logits[mask] = -10000\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    start_probs = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "    end_probs = torch.nn.functional.softmax(end_logits, dim=-1)[0]\n",
    "\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    scores = torch.triu(scores)\n",
    "\n",
    "    max_index = scores.argmax().item()\n",
    "    start_index = max_index // scores.shape[1]\n",
    "    end_index = max_index % scores.shape[1]\n",
    "    score = scores[start_index, end_index].item()\n",
    "\n",
    "    inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "    offsets = inputs_with_offsets['offset_mapping']\n",
    "\n",
    "    start_char, _ = offsets[start_index]\n",
    "    _, end_char = offsets[end_index]\n",
    "    answer = context[start_char:end_char]\n",
    "\n",
    "    return answer, score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "faffdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "?tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "80e89575",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(q3, text3,\n",
    "                   max_length=512,\n",
    "                   truncation='only_second',\n",
    "                   stride=30,\n",
    "                   return_overflowing_tokens=True,\n",
    "                   padding='longest',\n",
    "                   return_offsets_mapping = True)\n",
    "#_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "#offsets = inputs.pop(\"offset_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "81cad4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1cdeb4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(inputs['input_ids'])):\n",
    "    model_inputs = {'input_ids': inputs['input_ids'][i], \n",
    "                'token_type_ids': inputs['token_type_ids'][i],\n",
    "               'attention_mask': inputs['attention_mask'][i]}.convert_to_tensors(\"pt\")\n",
    "    outputs = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d48f831a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'convert_to_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tv/8qzgqvq554vgrs4zb6bwbwlm0000gn/T/ipykernel_2631/256417813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model_inputs = {'input_ids': inputs['input_ids'][i], \n\u001b[0m\u001b[1;32m      3\u001b[0m                 \u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                'attention_mask': inputs['attention_mask'][i]}.convert_to_tensors(\"pt\")\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'convert_to_tensors'"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "model_inputs = {'input_ids': inputs['input_ids'][i], \n",
    "                'token_type_ids': inputs['token_type_ids'][i],\n",
    "               'attention_mask': inputs['attention_mask'][i]}.convert_to_tensors(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "8bf61fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a389b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(question, context):\n",
    "    inputs = tokenizer(question, context, max_length=512, truncation='only_second',\n",
    "                       stride=30, return_overflowing_tokens=True, padding='longest',\n",
    "                       return_offset_mappings = True)\n",
    "    _ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offsets = inputs.pop(\"offset_mapping\")\n",
    "    inputs = inputs.convert_to_tensors('pt')\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    sequence_ids = inputs.sequence_ids()\n",
    "\n",
    "    # Mask out-of-context tokens\n",
    "    mask = [i != 1 for i in sequence_ids]\n",
    "\n",
    "    # Unmask the [CLS] token\n",
    "    mask[0] = False\n",
    "    mask = torch.tensor(mask)[None]\n",
    "\n",
    "    # Neg large nr's exp will be zero\n",
    "    start_logits[mask] = -10000\n",
    "    end_logits[mask] = -10000\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    start_probs = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "    end_probs = torch.nn.functional.softmax(end_logits, dim=-1)[0]\n",
    "\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    scores = torch.triu(scores)\n",
    "\n",
    "    max_index = scores.argmax().item()\n",
    "    start_index = max_index // scores.shape[1]\n",
    "    end_index = max_index % scores.shape[1]\n",
    "    score = scores[start_index, end_index].item()\n",
    "\n",
    "    inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "    offsets = inputs_with_offsets['offset_mapping']\n",
    "\n",
    "    start_char, _ = offsets[start_index]\n",
    "    _, end_char = offsets[end_index]\n",
    "    answer = context[start_char:end_char]\n",
    "\n",
    "    return answer, score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee61a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "85fd79d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', 0.9126068949699402)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score('cuándo sucede el asado?', text3[-200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f5f0b",
   "metadata": {},
   "source": [
    "#### Convoluted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a016d771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "57e94cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "offset_mapping = inputs['offset_mapping'][i]\n",
    "model_inputs = {'input_ids': torch.Tensor(inputs['input_ids'][i]), \n",
    "                'token_type_ids': torch.Tensor(inputs['token_type_ids'][i]),\n",
    "               'attention_mask': torch.Tensor(inputs['attention_mask'][i])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6bad681e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tv/8qzgqvq554vgrs4zb6bwbwlm0000gn/T/ipykernel_2631/1615718426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1840\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "outputs = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "3e3f2a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "da6ab602",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tv/8qzgqvq554vgrs4zb6bwbwlm0000gn/T/ipykernel_2631/3056379873.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True, padding=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1840\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "#inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "outputs = model(**model_inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "sequence_ids = inputs2.sequence_ids()\n",
    "\n",
    "# Mask out-of-context tokens\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]\n",
    "\n",
    "# Neg large nr's exp will be zero\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "\n",
    "# Convert logits to probabilities\n",
    "start_probs = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probs = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a49c4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "?tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "41dde543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"\", Score: \"0.5675\"\n"
     ]
    }
   ],
   "source": [
    "#score[start_pos, end_pos] = start_probs[start_pos] * end_probs[end_pos]\n",
    "\n",
    "scores = start_probs[:, None] * end_probs[None, :]\n",
    "scores = torch.triu(scores)\n",
    "\n",
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "score = scores[start_index, end_index].item()\n",
    "\n",
    "#inputs_with_offsets = tokenizer(q0, text2, return_offsets_mapping=True)\n",
    "#offsets = inputs_with_offsets['offset_mapping']\n",
    "offset_mapping =  \n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answr = context[start_char:end_char]\n",
    "\n",
    "print(f'Answer: \"{answr}\", Score: \"{score:.4f}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed646a4",
   "metadata": {},
   "source": [
    "#### Streamlined Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "8ca0b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_qa(question, long_context):\n",
    "    inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride = 128,\n",
    "    max_length = 384,\n",
    "    padding = \"longest\",\n",
    "    truncation = \"only_second\",\n",
    "    return_overflowing_tokens = True,\n",
    "    return_offsets_mapping = True,\n",
    "    )\n",
    "    _ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    inputs = inputs.convert_to_tensors(\"pt\")\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    \n",
    "    sequence_ids = inputs.sequence_ids() # Mask everything but context tokens\n",
    "    mask = [i != 1 for i in sequence_ids] # Unmask the [CLS] token\n",
    "    mask[0] = False # Mask all the [PAD] tokens\n",
    "    mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "    start_logits[mask] = -10000\n",
    "    end_logits[mask] = -10000\n",
    "    \n",
    "    start_probs = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "    end_probs = torch.nn.functional.softmax(end_logits, dim=-1)\n",
    "    best_candidate, best_score, loc = 0, 0, 0\n",
    "    for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "        scores = start_probs[:, None] * end_probs[None, :]\n",
    "        idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "        start_idx = idx // scores.shape[0]\n",
    "        end_idx = idx % scores.shape[0]\n",
    "        score = scores[start_idx, end_idx].item()\n",
    "        if score > best_score:\n",
    "            best_score, best_candidate, i = score, (start_idx, end_idx, score), loc\n",
    "        loc += 1\n",
    "\n",
    "    return best_candidate, offsets, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "781ef277",
   "metadata": {},
   "outputs": [],
   "source": [
    "cand, offs, i = long_qa(q3, text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "079c20d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, list)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cand), type(offs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e6e803de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    for candidate, offset in zip(candidates, offsets[i]):\n",
    "        start_token, end_token, score = candidate\n",
    "        start_char, _ = offset[start_token]\n",
    "        _, end_char = offset[end_token]\n",
    "        answer = long_context[start_char:end_char]\n",
    "        result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "48ec1ad4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tv/8qzgqvq554vgrs4zb6bwbwlm0000gn/T/ipykernel_2631/346881381.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mstart_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlong_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_char\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_char\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "for c in cand:\n",
    "    , off in zip(cand, offs[i]):\n",
    "    start_token, end_token, score = c\n",
    "    start_char, _ = off[start_token]\n",
    "    _, end_char = off[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "4a800c63",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'zip' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tv/8qzgqvq554vgrs4zb6bwbwlm0000gn/T/ipykernel_2631/3269560608.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'zip' has no len()"
     ]
    }
   ],
   "source": [
    "can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265a559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd7dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f42e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep] *",
   "language": "python",
   "name": "conda-env-deep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
